{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Genome Data Analysis - PTC\n",
    "#### Written by Ryan V Moriarty, updated August 2023\n",
    "\n",
    "\n",
    "### Purpose:\n",
    "Merge and map whole-genome sequences, identify variants, and characterize the sequences in viral epitopes. Processes a folder hierarchy of FASTQ files and saves epitope sequences, counts and statistics files to a corresponding folder hierarchy. This script will generate a tsv file with nucleotide sequences and the number of times they are present in the data set. This has been edited from Aliota et al, 2018 to examine MHC-restricted SIV epitopes and identify SNVs. \n",
    "\n",
    "To make your life easier downstream, format your FASTQ file names as such:\n",
    "\n",
    "- animal-Xdpi-repX_L001_R1_001.fastq.gz\n",
    "\n",
    "\n",
    "Python packages needed: \n",
    "- samtools\n",
    "- os\n",
    "- pathlib\n",
    "- tempfile\n",
    "- shutil\n",
    "- subprocess\n",
    "- pandas \n",
    "- glob \n",
    "- re\n",
    "- Bio.Seq \n",
    "\n",
    "Additional requirements:\n",
    "- BBtools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify dataset arguments\n",
    "\n",
    "Specify variables that need to be changed depending on the dataset and machine configuration. After these parameters are specified, the rest of the notebook should be runnable automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to FASTQ files to process\n",
    "# expects R1 and R2 paired samples\n",
    "FASTQ_FOLDER_PATH = \"/Volumes/titmouse/ViralEvolutionART/raw_fastqs\"\n",
    "exp_id = \"WGS\" # For naming epitope .txt files \n",
    "\n",
    "ANIMAL_PATTERN = \"cy\\d{4}\" # Not all animals have the same naming conventioin/name length\n",
    "\n",
    "# path to reference files for read orientation\n",
    "REF_BASE = '/Volumes/titmouse/ViralEvolutionART/ReferenceFiles'\n",
    "BBMAP_PATH = REF_BASE + '/bbmap'\n",
    "REF_FASTA = REF_BASE + '/M33262_barcode_withends.fasta'\n",
    "EPITOPES=True\n",
    "\n",
    "import pandas as pd\n",
    "epitope_seqs = pd.read_csv(REF_BASE + \"/pre_and_post_epitope_seqs.csv\")\n",
    "epitope_seqs['epitopeLength'] = epitope_seqs['epitopeLength'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for epitope identification\n",
    "\n",
    "Define functions that will be used in the workflow to count epitope sequences from FASTQ files. These functions have not been changed since they were used as published in Aliota et al, 2018, aside from commenting out the print_status command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_file():\n",
    "    '''create named temporary file\n",
    "    return temporary file object [0] and path to temporary file [1]'''\n",
    "\n",
    "    import pathlib\n",
    "    import tempfile\n",
    "    \n",
    "    temp = tempfile.NamedTemporaryFile()    \n",
    "    return [temp, temp.name]\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    '''create directory at specified location if one does not already exist'''\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "    return dir_path\n",
    "\n",
    "def find_files_in_path(search_path):\n",
    "    '''get list of files matching files in search_path'''\n",
    "    import glob\n",
    "    \n",
    "    f = glob.glob(search_path, recursive=True)\n",
    "    \n",
    "    return f\n",
    "\n",
    "def derive_file_name(source_file, find_string, replace_string):\n",
    "    '''create filename string '''\n",
    "    \n",
    "    f = source_file.replace(find_string, replace_string)\n",
    "    \n",
    "    return f\n",
    "\n",
    "def run_command(cmd_list, stdout_file = None, stderr_file = None):\n",
    "    '''run command with subprocess.call\n",
    "    if stdout or stderr arguments are passed, save to specified file\n",
    "    '''\n",
    "    \n",
    "    import subprocess\n",
    "    \n",
    "    #print(cmd_list)\n",
    "        \n",
    "    # if neither stdout or stderr specified\n",
    "    if stdout_file is None and stderr_file is None:\n",
    "        subprocess.call(cmd_list, stdout=subprocess.DEVNULL) \n",
    "        #RVM added subprocess.DEVNULL so it would stop printing the java command \n",
    "     \n",
    "    # if only stdout is specified\n",
    "    elif stdout_file is not None and stderr_file is None:\n",
    "        with open(stdout_file, 'w') as so:\n",
    "            subprocess.call(cmd_list, stdout = so)\n",
    "     \n",
    "    # if only stderr is specified\n",
    "    elif stdout_file is None and stderr_file is not None:\n",
    "        with open(stderr_file, 'w') as se:\n",
    "            subprocess.call(cmd_list, stderr = se)\n",
    "     \n",
    "    # if both stdout and stderr are specified\n",
    "    elif stdout_file is not None and stderr_file is not None:\n",
    "        with open(stdout_file, 'w') as so:\n",
    "            with open(stderr_file, 'w') as se:\n",
    "                subprocess.call(cmd_list, stdout = so, stderr = se)\n",
    "    \n",
    "    else: pass\n",
    "    \n",
    "def split_path(source_file):\n",
    "    '''get path to enclosing folder and basename for source_file'''\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    source_file_path = os.path.dirname(source_file)\n",
    "    source_file_basename = os.path.basename(source_file)\n",
    "    \n",
    "    return [source_file_path, source_file_basename]\n",
    "\n",
    "def create_barcode_tsv(two_column_barcode_count_file, three_column_barcode_count_file, sample_name):\n",
    "    '''add a column with sample_name to barcode count TSV output by kmercountexact'''\n",
    "    \n",
    "    import pandas as pd \n",
    "    # import file containing counts to pandas dataframe    \n",
    "    df = pd.read_csv(two_column_barcode_count_file, sep='\\t', names=['barcode_sequence', 'barcode_count'])\n",
    "    \n",
    "    # add column with sample name\n",
    "    df.insert(0, 'sample_name', sample_name)\n",
    "    \n",
    "    # export TSV with sample names added\n",
    "    df.to_csv(three_column_barcode_count_file, sep='\\t', index=False)\n",
    "    \n",
    "def remove_file(source_file):\n",
    "    '''remove file'''\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    os.remove(source_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for Epitope Identification \n",
    "\n",
    "Count unique epitope sequences in FASTQ sequences by:\n",
    "\n",
    "1. Make dictionary of samples to process.\n",
    "    - The key is the sample directory path and the value is the sample name (ex. cy1035-14dpi-rep1)\n",
    "    - Merge the R1 and R2 for each replicate to create combiined R1 and R2 fastq files. \n",
    "    \n",
    "1. Iterate over each sample.  \n",
    "1. Determine sequencing method by identifying the index sequence in the FASTQ header.  \n",
    "1. Merge and quality trim samples \n",
    "1. Map the merged whole-genome fastq file to the SIVmac239M reference using bbmap.\n",
    "1. Loop through each epitope if EPITOPES is set to \"True\"\n",
    "1. Extracting reads that contain the epitope\n",
    "    - this is done by searching for the upstream and downstream sequences (allowing for a 1bp mismatch).\n",
    "1. Orient reads in same direction\n",
    "    - the file is re-mapped to SIVmac239 using bbmap. The plus and minus strands are split using the \"splitsam\" utility of bbmap. The minus strand is reverse complemented and the two fastq files are concatenated.  \n",
    "1. Trim reads to contain only the epitope\n",
    "    - Using bbduk, the  upstream and downstream flanking sequence are removed, again allowing for a 1 bp mismatch. By orienting the reads in Step 6, the upstream sequence can be left-trimmed and the downstream sequence can be right-trimmed without needing to account for reverse complements.\n",
    "1. Count number of times each epitope appears\n",
    "    - Only sequences exactly matching the nucleotide length (i.e. 24bp for an 8mer) are included to exclude any PCR chimeras that are detected. \n",
    "1. Create TSV file with counts for each epitope sequence. This .txt file can then be processed further. \n",
    "    - Temporary files generated during this process are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of sample names, where the key is the sample directory path and the value is the file name sans forward/reverse designator and file extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Make dictionary of samples to process ##\n",
    "FASTQ_R1 = find_files_in_path(FASTQ_FOLDER_PATH + '/*R1*.fastq.gz')\n",
    "\n",
    "SAMPLES = {}\n",
    "for i in FASTQ_R1:\n",
    "    \n",
    "    # get path to directory containing sample FASTQ\n",
    "    SAMPLE_DIR = split_path(i)[0]\n",
    "    \n",
    "    # get sample name\n",
    "    FASTQ_R1_BASENAME = split_path(i)[1]\n",
    "    SAMPLE_NAME = re.sub('_S\\d{1,2}_L001_|R1_001.fastq.gz', '', FASTQ_R1_BASENAME) # RVM adjusted March 2023\n",
    "    #SAMPLE_NAME = derive_file_name(FASTQ_R1_BASENAME, '_R1_001.fastq.gz', '')\n",
    "    #print(SAMPLE_NAME[:SAMPLE_NAME.index('_S')]) \n",
    "    \n",
    "    SAMPLES[i] = (SAMPLE_DIR, SAMPLE_NAME)\n",
    "print(SAMPLES) # If you want to verify what samples are there and the absolute paths \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over each sample and merge paired reads, map to reference, and find epitope sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### 2. Iterate over every sample to process and: ##\n",
    "for key,val in SAMPLES.items():\n",
    "\n",
    "    # get sample path and sample name\n",
    "    SAMPLE_PATH = val[0]\n",
    "    SAMPLE_NAME = val[1]\n",
    "\n",
    "    R1 = key\n",
    "    R2 = re.sub(\"L001_R1_001\", \"L001_R2_001\", R1)\n",
    "    \n",
    "    # Alert uer to where you are in the loops \n",
    "    print(\"Processing sample \" + str(list(SAMPLES.keys()).index(key) + 1 ) + \" of \" + str(len(SAMPLES)) + \": \" + SAMPLE_NAME) \n",
    "   \n",
    "    # create directory to hold statistics files\n",
    "    STATS_PATH = create_directory(SAMPLE_PATH + '/stats')\n",
    "    MERGED_FQ_PATH = create_directory(FASTQ_FOLDER_PATH + '/merged_fastqs')\n",
    "\n",
    "    ## 3. Quality trimming and merging R1 and R2 FASTQ sequences ## \n",
    "    print(\"Merging FASTQ files\")\n",
    "    run_command([BBMAP_PATH + '/bbmerge.sh',  \n",
    "                'in=' + R1, \n",
    "                'in2=' + R2,\n",
    "                'out=' + MERGED_FQ_PATH + '/' + SAMPLE_NAME + '.merged.fastq.gz',\n",
    "                'qtrim=t',\n",
    "                'ow=t'],\n",
    "                stderr_file = STATS_PATH + '/' + SAMPLE_NAME + '.merging.stats.txt')\n",
    "        \n",
    "    print(\"Mapping full genome\")\n",
    "    OUTPUT_BAM = create_directory(FASTQ_FOLDER_PATH + '/bamfiles')\n",
    "    run_command([BBMAP_PATH + '/bbmap.sh',\n",
    "                    'in=' + MERGED_FQ_PATH + '/' + SAMPLE_NAME + '.merged.fastq.gz',\n",
    "                    'outm=' + OUTPUT_BAM + '/' + SAMPLE_NAME + '.fullmapped.sam',\n",
    "                    'ref=' + REF_FASTA,\n",
    "                    'ow=t'],\n",
    "                    stderr_file=STATS_PATH + '/' + SAMPLE_NAME + '.fullmapping.stats.txt')\n",
    "    \n",
    "    #print(\"Converting to BAM file, indexing, and generating coverage file\")\n",
    "    # Samtools must be version 1.9 for quasitools to later work \n",
    "    #run_command(['samtools', 'sort', \n",
    "    #             OUTPUT_BAM + '/' + SAMPLE_NAME + '.fullmapped.sam', \n",
    "    #             '-o',\n",
    "    #            OUTPUT_BAM + '/' + SAMPLE_NAME + '.bam'])\n",
    "    \n",
    "    #run_command(['samtools', 'index', '-b', OUTPUT_BAM + '/' + SAMPLE_NAME + '.bam'])\n",
    "\n",
    "\n",
    "    if EPITOPES == True:\n",
    "        EPITOPE_OUT_PATH = create_directory(FASTQ_FOLDER_PATH + '/EpitopeFiles')\n",
    "        print(\"Finding Epitope Changes\")\n",
    "        ## 4. Extracting reads that contain the barcode ##\n",
    "        for e in range(len(epitope_seqs)): # RVM added this loop \n",
    "\n",
    "            UPSTREAM_FLANKING = epitope_seqs['preEpitopeSeq'][e]\n",
    "            DOWNSTREAM_FLANKING = epitope_seqs['PostEpitopeSeq'][e]\n",
    "            EPITOPE_NAME = epitope_seqs['EpitopeName'][e]\n",
    "\n",
    "            print(\"Starting epitope: \" + EPITOPE_NAME)\n",
    "\n",
    "            # extract reads containing barcode\n",
    "            # bbduk command to find reads containing upstream flanking sequence   \n",
    "            #print(\"extracting reads\")\n",
    "            run_command([BBMAP_PATH + '/bbduk.sh',\n",
    "                          'in=' + MERGED_FQ_PATH + '/' + SAMPLE_NAME + '.merged.fastq.gz',\n",
    "                          'outm=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.containing_upstream_flank.fastq.gz',\n",
    "                          'literal=' + UPSTREAM_FLANKING,\n",
    "                          'k=20',\n",
    "                          'hdist=1',\n",
    "                          'ow=t'],\n",
    "                          stderr_file = STATS_PATH + '/' + SAMPLE_NAME + '-' + EPITOPE_NAME + '.containing_upstream_flank.stats.txt')\n",
    "\n",
    "            # bbduk command to find reads containing downstream flanking sequence \n",
    "            # (among those that contain upstream flanking sequence)\n",
    "            run_command([BBMAP_PATH + '/bbduk.sh',\n",
    "                          'in=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.containing_upstream_flank.fastq.gz',\n",
    "                          'outm=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.containing_barcode.fastq.gz',\n",
    "                          'literal=' + DOWNSTREAM_FLANKING,\n",
    "                          'k=20',\n",
    "                          'hdist=1',\n",
    "                          'ow=t'],\n",
    "                          stderr_file=STATS_PATH + '/' + SAMPLE_NAME + '-' + EPITOPE_NAME + '.containing_barcode.stats.txt')\n",
    "\n",
    "            ## 5. Orient reads in same direction ##\n",
    "            #print(\"orienting reads\")\n",
    "\n",
    "            # orient reads by mapping to reference\n",
    "            # use SIV reference with accession M33262\n",
    "            run_command([BBMAP_PATH + '/bbmap.sh',\n",
    "                        'in=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.containing_barcode.fastq.gz',\n",
    "                        'outm=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.mapped.sam',\n",
    "                        'ref=' + REF_FASTA,\n",
    "                        'ow=t'],\n",
    "                        stderr_file=STATS_PATH + '/' + SAMPLE_NAME + '-' + EPITOPE_NAME + '.mapping.stats.txt')\n",
    "\n",
    "            # split mapped SAM file into plus, minus, and unmapped strand SAM temporary files\n",
    "            PLUS_STRAND_SAM = create_temp_file()\n",
    "            MINUS_STRAND_SAM = create_temp_file()\n",
    "            UNMAPPED_STRAND_SAM = create_temp_file()\n",
    "\n",
    "            #print(\"Splitting sam\")\n",
    "            run_command([BBMAP_PATH + '/splitsam.sh',\n",
    "                        EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.mapped.sam',\n",
    "                        PLUS_STRAND_SAM[1] + '.sam',\n",
    "                        MINUS_STRAND_SAM[1] + '.sam',\n",
    "                        UNMAPPED_STRAND_SAM[1] + '.sam'])    \n",
    "\n",
    "            # reformat plus and minus strand SAM files to FASTQ\n",
    "            PLUS_STRAND_FASTQ = create_temp_file()\n",
    "            MINUS_STRAND_FASTQ = create_temp_file()    \n",
    "\n",
    "            #print(\"reformatting files - plus strand\")\n",
    "            run_command([BBMAP_PATH + '/reformat.sh',\n",
    "                        'in=' + PLUS_STRAND_SAM[1] + '.sam',\n",
    "                        'out=' + PLUS_STRAND_FASTQ[1] + '.fastq.gz'])\n",
    "            #print(\"reformatting files - minus strand\")\n",
    "            run_command([BBMAP_PATH + '/reformat.sh',\n",
    "                        'in=' + MINUS_STRAND_SAM[1] + '.sam',\n",
    "                        'out=' + MINUS_STRAND_FASTQ[1] + '.fastq.gz',\n",
    "                        'rcomp=t'])\n",
    "\n",
    "            # concatenate oriented plus and minus strand FASTQ\n",
    "            run_command(['cat',\n",
    "                        PLUS_STRAND_FASTQ[1] + '.fastq.gz',\n",
    "                        MINUS_STRAND_FASTQ[1] + '.fastq.gz'],\n",
    "                        stdout_file = EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.oriented.fastq.gz')\n",
    "\n",
    "            ## 6. Trim reads to contain only the barcode ##\n",
    "            # remove upstream flanking sequence from oriented reads\n",
    "            UPSTREAM_FLANK_REMOVED = create_temp_file()\n",
    "\n",
    "            run_command([BBMAP_PATH + '/bbduk.sh',\n",
    "                       'in=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.oriented.fastq.gz',\n",
    "                       'out=' + UPSTREAM_FLANK_REMOVED[1] + '.fastq.gz',\n",
    "                       'literal=' + UPSTREAM_FLANKING,\n",
    "                       'ktrim=l',\n",
    "                       'rcomp=f',\n",
    "                       'k=20',\n",
    "                       'hdist=1',\n",
    "                       'ow=t'],\n",
    "                       stderr_file = STATS_PATH + '/' + SAMPLE_NAME + '-' + EPITOPE_NAME + '.upstream.flank.removal.stats.txt')\n",
    "\n",
    "            # remove downstream flanking sequence from oriented reads\n",
    "            run_command([BBMAP_PATH + '/bbduk.sh',\n",
    "                       'in=' + UPSTREAM_FLANK_REMOVED[1] + '.fastq.gz',\n",
    "                       'out=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.barcodes.fastq.gz',\n",
    "                       'literal=' + DOWNSTREAM_FLANKING,\n",
    "                       'ktrim=r',\n",
    "                       'rcomp=f',\n",
    "                       'k=20',\n",
    "                       'hdist=1',\n",
    "                       'minlength=24', # initially these were set to 27 (for a 9mer epitope)\n",
    "                       'maxlength=33', # This is now set to look for 8-11mers \n",
    "                       'ow=t'],\n",
    "                       stderr_file = STATS_PATH + '/' + SAMPLE_NAME + '-' + EPITOPE_NAME + '.downstream.flank.removal.stats.txt')\n",
    "\n",
    "\n",
    "            ## 7. Count number of times each barcode occurs ##\n",
    "\n",
    "            # count number of identical barcodes in each sample\n",
    "            # create temporary count file\n",
    "            BARCODE_COUNT_TEMP = create_temp_file()\n",
    "\n",
    "            run_command([BBMAP_PATH + '/kmercountexact.sh',\n",
    "                  'in=' + EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.barcodes.fastq.gz',\n",
    "                  'out=' + BARCODE_COUNT_TEMP[1] + '.txt',\n",
    "                  'fastadump=f',\n",
    "                  'k=' + epitope_seqs['epitopeLength'][e], # Added by RVM because of different epitope lengths\n",
    "                  'rcomp=f',\n",
    "                  'ow=t'])\n",
    "\n",
    "            ## 8. Create TSV file with barcode counts for each sequence ##\n",
    "            # create three column barcode count file with sample name\n",
    "\n",
    "            create_barcode_tsv(BARCODE_COUNT_TEMP[1] + '.txt',\n",
    "                               EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '-' + EPITOPE_NAME +'.barcode_counts.txt',\n",
    "                               SAMPLE_NAME)\n",
    "\n",
    "            #print(\"Epitope \" + EPITOPE_NAME + \" for sample \" + SAMPLE_NAME + \" txt file is complete!\")\n",
    "\n",
    "            # remove temporary files\n",
    "            PLUS_STRAND_SAM[0].close()\n",
    "            PLUS_STRAND_FASTQ[0].close()\n",
    "            MINUS_STRAND_SAM[0].close()\n",
    "            MINUS_STRAND_FASTQ[0].close()\n",
    "            UNMAPPED_STRAND_SAM[0].close()\n",
    "            UPSTREAM_FLANK_REMOVED[0].close()\n",
    "            BARCODE_COUNT_TEMP[0].close()\n",
    "\n",
    "            # remove intermediate files\n",
    "            remove_file(EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.containing_upstream_flank.fastq.gz')\n",
    "            remove_file(EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.containing_barcode.fastq.gz')\n",
    "            remove_file(EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.oriented.fastq.gz')\n",
    "            remove_file(EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.barcodes.fastq.gz')\n",
    "            #this is the end of the epitope-specific loop \n",
    "\n",
    "        remove_file(EPITOPE_OUT_PATH + '/' + SAMPLE_NAME + '.mapped.sam')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the barcode .txt files to organized .csv files \n",
    "### Functions used for .txt file reorganization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_files(files):\n",
    "    \"\"\" Take the list of files that were generated through a looped version of something Dave made, then \n",
    "        make a list of files that have sufficient (arbitrary) amount of reads, mark those that have epitopes\n",
    "        but don't make thresholds, and those that are completely empty\n",
    "        \n",
    "        Args: \n",
    "            files = list of .txt files output \n",
    "        Returns:\n",
    "            four items: one list of the files that had sufficient njumber of epitopes, one that has files with\n",
    "                low number of sequences, one of empties, and then one that lists how many sequences are in each\n",
    "                and every file so we can plot it.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd \n",
    "    \n",
    "    contains_seqs = []\n",
    "    empties = []\n",
    "    low_seqs = []\n",
    "    num_seqs = []\n",
    "    for f in range(len(files)):\n",
    "        df = pd.read_csv(files[f], sep = '\\t')\n",
    "        num_seqs.append(int(pd.DataFrame.sum(df['barcode_count'])))\n",
    "        if len(df) > 0:\n",
    "            if pd.DataFrame.sum(df['barcode_count']) > 1000:\n",
    "                contains_seqs.append(files[f])\n",
    "            else:\n",
    "                low_seqs.append(files[f])\n",
    "        else:\n",
    "            empties.append(files[f])\n",
    "    print(\"There are \" + str(len(contains_seqs)) + \" files with 1000+ epitope sequences detected, \\n\" + \n",
    "          str(len(low_seqs)) + \" with <1000 epitope sequences detected, \\nand \" + \n",
    "          str(len(empties)) + \" empty files.\")\n",
    "    return contains_seqs, low_seqs, empties, num_seqs\n",
    "\n",
    "def identify_animals(files):\n",
    "    \"\"\"Let's figure out which animals have sequences with epitopes so we can later make a list of them\n",
    "        \n",
    "       Args:\n",
    "           files = list of .txt files from your data set \n",
    "       \n",
    "       Returns:\n",
    "           list of animals that have been sequenced\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    animals = set()\n",
    "    for f in files:\n",
    "        animals.add(re.search(ANIMAL_PATTERN, f)[0])\n",
    "    return list(animals)\n",
    "\n",
    "def translate_and_group(file):\n",
    "    \"\"\"Since we care more about the effet on the translated product, translate the sequences and then combine \n",
    "       identical amino acid sequences.\n",
    "       \n",
    "       Args: \n",
    "           file = the file you want translated, in .txt form\n",
    "        \n",
    "       Returns:\n",
    "           pandas data frame with translated sequences and corresponding counts\n",
    "    \"\"\"\n",
    "    from Bio import Seq \n",
    "    \n",
    "    df = pd.read_csv(file, sep = '\\t')\n",
    "    nts = []\n",
    "    aas = []\n",
    "    for seq in df['barcode_sequence']:\n",
    "        if len(seq) % 3 == 0:\n",
    "            nts.append(seq)\n",
    "            transl = Bio.Seq.translate(seq)\n",
    "            aas.append(transl)\n",
    "        else:\n",
    "            print(file + \" has a nt sequence length of \" + str(len(seq)) + \": \" + seq)\n",
    "\n",
    "    df['barcode_sequence'] = pd.DataFrame(aas)\n",
    "    df.insert(1, 'nt_sequence', nts)\n",
    "    \n",
    "    return df.groupby(['barcode_sequence', 'nt_sequence'], as_index=False).sum() \n",
    "\n",
    "def sort_by_animal(transl_files, animal_list):\n",
    "    \"\"\"Make lists of all files from each animal and return one list of sorted lists. \n",
    "       Args:\n",
    "           transl_files = made from translate_and_group, list of translated sequences and the counts\n",
    "           animal_list = made from identify_animals, sets up the unique animals in the data set \n",
    "       Returns:\n",
    "           sorted list of lists for each unique animal \n",
    "    \"\"\"\n",
    "    import re \n",
    "    \n",
    "    sorted_list = []\n",
    "    for a in animal_list:\n",
    "        byanimal = []\n",
    "        for t in transl_files:\n",
    "            if re.search(ANIMAL_PATTERN, t[0])[0] == a:\n",
    "                byanimal.append(t)\n",
    "        sorted_list.append(byanimal)\n",
    "    return sorted_list\n",
    "\n",
    "def write_translated_csv(sorted_file):\n",
    "    \"\"\"Take a file from the sorted list and write it as a .csv \n",
    "    \n",
    "        Args:\n",
    "            sorted_list_name = file from the list, sorted by animal, that you want replicate from \n",
    "        \n",
    "        Returns:\n",
    "            csv of the translated file \n",
    "    \"\"\"\n",
    "    import re \n",
    "    \n",
    "    an_id = re.search(ANIMAL_PATTERN, sorted_file[0])\n",
    "    epitope = sorted_file[0].split('-')[3]\n",
    "    filename = str(an_id) +  \"_\" + str(epitope[0:epitope.index('.')]) + \"_\" + exp_id + \".csv\"\n",
    "    sorted_file[1].to_csv(filename)\n",
    "    \n",
    "def find_wildtype(dataframe):\n",
    "    \"\"\"Since we don't know what the wild type (SIVmac239) epitope sequences are off the top of our head, \n",
    "       we can go through and identify which epitopes are and are not the wild type sequence \n",
    "        \n",
    "        Args: \n",
    "            dataframe = a pandas data frame that has been filtered based on frequency\n",
    "            \n",
    "        Returns: \n",
    "            a data frame with the wild type epitope indicated, with a row added to the \n",
    "            data frame in the case that the wild type sequence is not present \n",
    "    \n",
    "    \"\"\"\n",
    "    epitope_id = dataframe['epitope'][0]\n",
    "    epitopes_found = dataframe['barcode_sequence'].tolist()\n",
    "    wt_seq = epitope_seqs[epitope_seqs['EpitopeName'] == epitope_id]['EpitopeSeq'].tolist()[0]\n",
    "    \n",
    "    nts_found = dataframe['nt_sequence'].tolist()\n",
    "    wt_nts = epitope_seqs[epitope_seqs['EpitopeName'] == epitope_id]['epitopeNtSeq'].tolist()[0]\n",
    "    wt_nts = wt_nts.replace(\"U\", \"T\")\n",
    "    \n",
    "    if wt_seq in epitopes_found:\n",
    "        # If there are mutations in the WT nucleotide sequence, we need to make sure that all of the WT epitope seqs are identified\n",
    "        wt_indices = [i for i,d in enumerate(epitopes_found) if d==wt_seq]\n",
    "        for w in wt_indices:\n",
    "            new_seq = dataframe.loc[w, 'barcode_sequence'] + \" (WT)\"\n",
    "            dataframe.loc[w, 'barcode_sequence'] = new_seq\n",
    "            \n",
    "    else:\n",
    "        wt_index = len(epitopes_found)\n",
    "        wt_row = {'barcode_sequence': wt_seq + \" (WT)\", \n",
    "                         'nt_sequence': wt_nts, \n",
    "                         'barcode_count': 0,\n",
    "                         'animal': dataframe['animal'][0],\n",
    "                         'sample': dataframe['sample'][0],\n",
    "                         'epitope':epitope_id,\n",
    "                         'dpi':dataframe['dpi'][0],\n",
    "                         'rep':dataframe['rep'][0],\n",
    "                         'freq':0}\n",
    "        dataframe = dataframe.append(wt_row, ignore_index = True )\n",
    "        \n",
    "    if wt_nts in nts_found:\n",
    "        nt_index = nts_found.index(wt_nts)\n",
    "        new_seq = dataframe.loc[w, 'nt_sequence'] + \" (WT)\"\n",
    "        dataframe.loc[w, 'nt_sequence'] = new_seq\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate epitope sequences, write translated .csv files, and filter out low-frequency epitope sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio \n",
    "import glob\n",
    "import re\n",
    "\n",
    "txt_files = [f for f in glob.glob(FASTQ_FOLDER_PATH + \"/EpitopeFiles/*.txt\")]\n",
    "viable_files = remove_empty_files(txt_files)[0]\n",
    "\n",
    "translated_files = []\n",
    "for v in viable_files:\n",
    "    translated_files.append((v, translate_and_group(v)))\n",
    "\n",
    "animals = identify_animals(txt_files)\n",
    "sorted_list = sort_by_animal(translated_files, animals)\n",
    "for s in sorted_list:\n",
    "    for t in s:\n",
    "        t_sub = t[0][t[0].rfind(\"/\") +1:]\n",
    "        t[1]['epitope'] = t_sub[t_sub.rfind(\"-\")+1:t_sub.index(\"barcode_counts.txt\")-1]\n",
    "        t[1]['dpi'] = \"None\" if re.search(\"d\\d{1,3}\", t_sub) is None and re.search(\"\\d{1,3}dpi\", t_sub) is None else re.search(\"d\\d{1,3}|\\d{1,3}dpi\", t_sub)[0]\n",
    "        t[1]['sample'] = t_sub[t_sub.index(\"-\")+1:t_sub.index(\"rep\")-1]\n",
    "        t[1]['rep'] = re.search(\"rep\\d{1}\", t_sub)[0]\n",
    "        t[1]['freq'] = t[1]['barcode_count']/t[1]['barcode_count'].sum()\n",
    "        t[1]['animal'] = re.search(ANIMAL_PATTERN, t_sub)[0]\n",
    "        t[1].sort_values('barcode_count', ascending=False)\n",
    "        t[1].to_csv(t[0][:t[0].index(\".barcode\")] + \"_\" + exp_id + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are filtering out any epitope sequence that comprises less than 1% of the total frequency. These will be \n",
    "# grouped into an \"other\" group. The wild type (SIVmac239) sequence is also indicated. \n",
    "sorted_list = sort_by_animal(translated_files, animals)\n",
    "\n",
    "filtered_list = []\n",
    "merged_list = []\n",
    "for s in sorted_list[0:1]:\n",
    "    filtered_animal = []\n",
    "    for t in s:\n",
    "        df = t[1].loc[t[1]['freq'] > 0.01].copy()\n",
    "        other_row = {'barcode_sequence': 'other_0.01', \n",
    "                     'nt_sequence': 'other_0.01', \n",
    "                     'barcode_count': sum(t[1]['barcode_count']) - sum(df['barcode_count']),\n",
    "                     'animal': t[1]['animal'][0],\n",
    "                     'epitope': t[1]['epitope'][0],\n",
    "                     'dpi': t[1]['dpi'][0],\n",
    "                     'sample': t[1]['sample'][0],\n",
    "                     'rep': t[1]['rep'][0],\n",
    "                     'freq': 1 - sum(df['freq'])\n",
    "                    }\n",
    "        df = df.append(pd.Series(other_row), ignore_index=True)\n",
    "        filtered_name = t[0][0:t[0].index(\".\")] + \"_\" + exp_id + \".filtered.csv\"\n",
    "        df = find_wildtype(df)\n",
    "        df.to_csv(filtered_name)\n",
    "        filtered_animal.append(df)\n",
    "    \n",
    "    if len(filtered_animal) > 0:\n",
    "        merged_animal = pd.concat(filtered_animal)\n",
    "        merged_animal_name = FASTQ_FOLDER_PATH + \"/EpitopeFiles/\" + t[1]['animal'][0] + \"_\" + exp_id + \".merged.filtered.csv\" \n",
    "        merged_animal.to_csv(merged_animal_name)\n",
    "        merged_list.append(merged_animal)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
